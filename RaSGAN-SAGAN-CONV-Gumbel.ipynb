{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relativistic average GAN for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from viz import updatable_display2\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from generators import GumbelSAGenerator\n",
    "from discriminators import GumbelSADiscriminator\n",
    "from utils import img2vec,vec2img,sample_sequence_noise,true_target,fake_target,onehot,num_parameters\n",
    "\n",
    "from timeSeries import Sinusoids\n",
    "from visualize import plotSamples\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_steps = 3\n",
    "dataset_size = 10000\n",
    "num_classes = 10\n",
    "\n",
    "data_loader = DataLoader(Sinusoids(num_steps,virtual_size=dataset_size,quantization=num_classes),batch_size=batch_size,shuffle=True)\n",
    "valid_data_loader = DataLoader(Sinusoids(num_steps,virtual_size=dataset_size,quantization=num_classes),batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GumbelSAGenerator(\n",
      "  (layers): Sequential(\n",
      "    (0): ConvTranspose1d(100, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose1d(512, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): SelfAttention(hidden_size = 256)\n",
      "    (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): ConvTranspose1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU()\n",
      "    (12): ConvTranspose1d(128, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      "  (gumbelsoftmax): GumbelSoftmax()\n",
      ")\n",
      "\n",
      "GumbelSADiscriminator(\n",
      "  (embeddings): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=128, bias=False)\n",
      "    (1): Linear(in_features=10, out_features=128, bias=False)\n",
      "    (2): Linear(in_features=10, out_features=128, bias=False)\n",
      "    (3): Linear(in_features=10, out_features=128, bias=False)\n",
      "    (4): Linear(in_features=10, out_features=128, bias=False)\n",
      "    (5): Linear(in_features=10, out_features=128, bias=False)\n",
      "  )\n",
      "  (layers): Sequential(\n",
      "    (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.2)\n",
      "    (4): SelfAttention(hidden_size = 256)\n",
      "    (5): LeakyReLU(negative_slope=0.2)\n",
      "    (6): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (7): LeakyReLU(negative_slope=0.2)\n",
      "    (8): Conv1d(512, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (9): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Number of parameters for G: 734411\n",
      "Number of parameters for D: 633026\n",
      "Number of parameters in total: 1367437\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "dropout_prob = 0.3\n",
    "noise_dim = 100\n",
    "output_size = num_classes\n",
    "\n",
    "num_test_samples = 100\n",
    "test_noise = sample_sequence_noise(num_test_samples,num_steps,noise_dim,device).view(num_test_samples,noise_dim,num_steps)\n",
    "\n",
    "# intialize models\n",
    "generator = GumbelSAGenerator(input_size=noise_dim,hidden_size=128,output_size=output_size,device=device).to(device)\n",
    "discriminator = GumbelSADiscriminator(input_size=output_size,hidden_size=128,output_size=1,num_embeddings=6).to(device)\n",
    "\n",
    "# otpimizers\n",
    "g_optimizer = optim.Adam(generator.parameters(),lr=lr)\n",
    "d_optimizer = optim.Adam(discriminator.parameters(),lr=lr)\n",
    "loss_fun = nn.BCEWithLogitsLoss()\n",
    "pretrain_loss_fun = nn.NLLLoss()\n",
    "\n",
    "# Create logger instance\n",
    "dis = updatable_display2(['train'],[\"epoch\",\"d_error\",\"g_error\",\"beta\"])\n",
    "pretrain_dis = updatable_display2(['train'],[\"pretrain epoch\",\"pretrain_g_error\"])\n",
    "# Total number of epochs to train\n",
    "num_epochs = 200\n",
    "\n",
    "\n",
    "print(generator)\n",
    "print()\n",
    "print(discriminator)\n",
    "np_g = num_parameters(generator)\n",
    "np_d = num_parameters(discriminator)\n",
    "print(\"Number of parameters for G: {}\\nNumber of parameters for D: {}\\nNumber of parameters in total: {}\"\n",
    "      .format(np_g,np_d,np_g+np_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(real_data,fake_data,optimizer):\n",
    "    '''\n",
    "    Train the generator to generate realistic samples and thereby fool the discriminator\n",
    "    '''\n",
    "    N = fake_data.size(0)\n",
    "    \n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1.1 Train on Real Data\n",
    "    c_x_r = discriminator(real_data)\n",
    "\n",
    "    # 1.2 Train on Fake Data\n",
    "    c_x_f = discriminator(fake_data)\n",
    "    \n",
    "    # compute the average of c_x_*\n",
    "    c_x_r_mean = torch.mean(c_x_r,dim=0)\n",
    "    c_x_f_mean = torch.mean(c_x_f,dim=0)\n",
    "    \n",
    "    losses_real = []\n",
    "    losses_fake = []\n",
    "    for i in range(c_x_r.size(1)):\n",
    "        losses_real.append(loss_fun(c_x_r[:,i,:]-c_x_f_mean[i,:],fake_target(N,device)))\n",
    "        losses_fake.append(loss_fun(c_x_f[:,i,:]-c_x_r_mean[i,:],true_target(N,device)))\n",
    "    loss_real = torch.stack(losses_real).mean()\n",
    "    loss_fake = torch.stack(losses_fake).mean()\n",
    "    \n",
    "    loss = (loss_real + loss_fake)/2.0\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(real_data,fake_data,optimizer):\n",
    "    '''\n",
    "    Train the discriminator to distinguish between real and fake data\n",
    "    '''\n",
    "    N = real_data.size(0)\n",
    "    \n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 1.1 Train on Real Data\n",
    "    c_x_r = discriminator(real_data)\n",
    "\n",
    "    # 1.2 Train on Fake Data\n",
    "    c_x_f = discriminator(fake_data)\n",
    "    \n",
    "    # compute the average of c_x_*\n",
    "    c_x_r_mean = torch.mean(c_x_r,dim=0)\n",
    "    c_x_f_mean = torch.mean(c_x_f,dim=0)\n",
    "    \n",
    "    losses_real = []\n",
    "    losses_fake = []\n",
    "    for i in range(c_x_r.size(1)):\n",
    "        losses_real.append(loss_fun(c_x_r[:,i,:]-c_x_f_mean[i,:],true_target(N,device)))\n",
    "        losses_fake.append(loss_fun(c_x_f[:,i,:]-c_x_r_mean[i,:],fake_target(N,device)))\n",
    "    loss_real = torch.stack(losses_real).mean()\n",
    "    loss_fake = torch.stack(losses_fake).mean()\n",
    "        \n",
    "    #loss_real = loss_fun(c_x_r-c_x_f_mean,true_target(N,device))\n",
    "    #loss_fake = loss_fun(c_x_f-c_x_r_mean,fake_target(N,device))\n",
    "    \n",
    "    loss = (loss_real + loss_fake)/2.0\n",
    "    loss.backward()\n",
    "    \n",
    "    # 1.3 Update weights with gradients\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_generator(real_data,fake_data,optimizer):\n",
    "    '''\n",
    "    Pretrain the generator to generate realistic samples for a good initialization\n",
    "    '''\n",
    "    num_classes = fake_data.size(2)\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    fake_data = torch.log(fake_data)\n",
    "    for i in range(fake_data.size(1)):\n",
    "        loss += pretrain_loss_fun(fake_data[:,i,:],real_data[:,i])\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_steps = 1\n",
    "gen_train_freq = 1\n",
    "max_temperature = torch.FloatTensor([2]).to(device)\n",
    "pretrain_temperature = torch.FloatTensor([1]).to(device)\n",
    "\n",
    "epochs_pretrain = 1000\n",
    "pretrain_step = 0\n",
    "\n",
    "# pretrain generator\n",
    "for ep in range(epochs_pretrain):\n",
    "    for n_batch,real_batch in enumerate(data_loader):\n",
    "        N = real_batch.size(0)\n",
    "        # 1. Train Discriminator\n",
    "        real_batch = real_batch.squeeze(2)\n",
    "        real_data = real_batch.to(device)\n",
    "\n",
    "        # Generate fake data\n",
    "        fake_data = generator(sample_sequence_noise(N,num_steps,noise_dim,device).view(N,noise_dim,num_steps),\n",
    "                              pretrain_temperature)\n",
    "        # Train G\n",
    "        pretrain_g_error = pretrain_generator(real_data,fake_data,g_optimizer)\n",
    "        \n",
    "        # Log batch error and delete tensors\n",
    "        pretrain_dis.update(pretrain_step,'train',{\"pretrain epoch\":ep,\"pretrain_g_error\":pretrain_g_error.item()})\n",
    "        pretrain_step += 1\n",
    "        if pretrain_step % 200 == 0:\n",
    "            test_samples = generator(test_noise,pretrain_temperature)\n",
    "            test_samples_vals = torch.argmax(test_samples,dim=2)\n",
    "            pretrain_dis.display(scale=True)\n",
    "            plotSamples(real_data.type(torch.FloatTensor),xu=num_steps,yu=num_classes,title=\"Real data\")\n",
    "            plotSamples(test_samples_vals,xu=num_steps,yu=num_classes,title=\"Fake data\")\n",
    "            plt.show()\n",
    "        del fake_data\n",
    "        del real_data\n",
    "    \n",
    "global_step = 0\n",
    "epoch = 0\n",
    "d_error = 0\n",
    "g_error = 0\n",
    "\n",
    "# train adverserially\n",
    "try:\n",
    "    while epoch < num_epochs:\n",
    "        temperature = max_temperature**((epoch+1)/num_epochs)\n",
    "        for n_batch,real_batch in enumerate(data_loader):\n",
    "            N = real_batch.size(0)\n",
    "            # 1. Train Discriminator\n",
    "            real_batch = onehot(real_batch.squeeze(2),num_classes).type(torch.FloatTensor)\n",
    "            real_data = real_batch.to(device)\n",
    "\n",
    "            # Generate fake data and detach \n",
    "            # (so gradients are not calculated for generator)\n",
    "            noise_tensor = sample_sequence_noise(N,num_steps,noise_dim,device).view(N,noise_dim,1)\n",
    "            with torch.no_grad():\n",
    "                fake_data = generator(noise_tensor,temperature).detach()\n",
    "            # Train D\n",
    "            d_error = train_discriminator(real_data,fake_data,d_optimizer)\n",
    "\n",
    "            # 2. Train Generator every 'gen_train_freq' steps\n",
    "            if global_step % gen_train_freq == 0:\n",
    "                for _ in range(gen_steps):\n",
    "                    # Generate fake data\n",
    "                    fake_data = generator(sample_sequence_noise(N,num_steps,noise_dim,device).view(N,noise_dim,1),temperature)\n",
    "                    # Train G\n",
    "                    g_error = train_generator(real_data,fake_data,g_optimizer)\n",
    "                    g_error = g_error.item()\n",
    "\n",
    "            # Log batch error and delete tensors\n",
    "            dis.update(global_step,'train',{\"epoch\":epoch,\"d_error\":d_error.item(),\"g_error\":g_error,\n",
    "                                            \"beta\":temperature.item()})\n",
    "            global_step += 1\n",
    "            del fake_data\n",
    "            del noise_tensor\n",
    "\n",
    "            # Display Progress every few batches\n",
    "            if global_step % 50 == 0:\n",
    "                test_samples = generator(test_noise,temperature)\n",
    "                test_samples_vals = torch.argmax(test_samples,dim=2)\n",
    "                dis.display(scale=True)\n",
    "                plotSamples(torch.argmax(real_data,dim=2),xu=num_steps,yu=num_classes,title=\"Real data\")\n",
    "                plotSamples(test_samples_vals,xu=num_steps,yu=num_classes,title=\"Fake data\")\n",
    "                if epoch % 50 == 0:\n",
    "                    plt.savefig(\"Figures/RaSGAN-SAGANCONV-ToyData-Epoch=\"+str(epoch)+\".png\")\n",
    "                plt.show()\n",
    "            del real_data\n",
    "        epoch += 1\n",
    "except:\n",
    "    test_samples = generator(test_noise,temperature)\n",
    "    test_samples_vals = torch.argmax(test_samples,dim=2)\n",
    "    dis.display(scale=True)\n",
    "    plotSamples(torch.argmax(real_data,dim=2),xu=num_steps,yu=num_classes,title=\"Real data\")\n",
    "    plotSamples(test_samples_vals,xu=num_steps,yu=num_classes,title=\"Fake data\")\n",
    "    plt.savefig(\"Figures/RaSGAN-SAGANCONV-ToyData.png\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
