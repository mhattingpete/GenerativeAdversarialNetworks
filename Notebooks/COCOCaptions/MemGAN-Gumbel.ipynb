{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN for Sequential data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does model save path exist: True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../Scripts\")\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from viz import updatable_display2\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "from utils import onehot,num_parameters,sample_noise,true_target,fake_target,save_model,load_model\n",
    "from generators import MemoryGenerator\n",
    "from discriminators import GumbelRNNDiscriminator\n",
    "\n",
    "from Dataloaders.load_coco_captions_dataset import create_dataset\n",
    "from visualize import plotSamples\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_save_path = os.path.join(\"../Saved_models\",\"MEM-GAN\")\n",
    "\n",
    "print(\"Does model save path exist:\",os.path.exists(model_save_path))\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.68s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.32s)\n",
      "creating index...\n",
      "index created!\n",
      "Dataset loaded\n",
      "Vocabuly build\n",
      "Vocabuly statistics\n",
      "\n",
      "Most common words in the vocabulary:\n",
      " [('a', 684546), ('on', 149490), ('of', 142745), ('the', 137947), ('in', 128745), ('with', 107681), ('and', 98721), ('is', 68632), ('man', 50780), ('to', 47713)]\n",
      "Size of the vocabulary: 2004\n",
      "Max sequence lenght 49\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "data_dict = create_dataset(\"../../../Datasets/COCO2015Captions\",batch_size,device,min_vocab_freq=100,max_vocab_size=None)\n",
    "train_data,val_data,test_data = data_dict[\"data_iters\"]\n",
    "text_field = data_dict[\"fields\"]\n",
    "num_classes = data_dict[\"num_classes\"]\n",
    "SOS_TOKEN,EOS_TOKEN,UNK_TOKEN,PAD_TOKEN = data_dict[\"tokens\"]\n",
    "max_seq_len = data_dict[\"max_seq_len\"]\n",
    "\n",
    "train_iter = iter(train_data)\n",
    "val_iter = iter(val_data)\n",
    "test_iter = iter(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters for G: 130385045\n",
      "Number of parameters for D: 514825\n",
      "Number of parameters in total: 130899870\n"
     ]
    }
   ],
   "source": [
    "g_lr = 1e-4\n",
    "d_lr = 1e-3\n",
    "\n",
    "noise_size = 128\n",
    "\n",
    "num_test_samples = 100\n",
    "test_noise = sample_noise(num_test_samples,noise_size,device)\n",
    "\n",
    "# intialize models\n",
    "generator = MemoryGenerator(hidden_size=256,noise_size=noise_size,output_size=num_classes,max_seq_len=max_seq_len,\n",
    "                               device=device,num_heads=32,similarity=nn.CosineSimilarity(dim=-1),\n",
    "                            SOS_TOKEN=SOS_TOKEN).to(device)\n",
    "discriminator = GumbelRNNDiscriminator(input_size=num_classes,hidden_size=256,output_size=1).to(device)\n",
    "\n",
    "# otpimizers\n",
    "g_optimizer = optim.Adam(generator.parameters(),lr=g_lr)\n",
    "d_optimizer = optim.Adam(discriminator.parameters(),lr=d_lr)\n",
    "g_lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(g_optimizer,mode=\"min\",factor=0.5,patience=2,verbose=True)\n",
    "\n",
    "# losses\n",
    "loss_fun = nn.BCEWithLogitsLoss()\n",
    "loss_weight = torch.ones(num_classes).to(device)\n",
    "loss_weight[SOS_TOKEN] = 0.0\n",
    "#loss_weight[EOS_TOKEN] = 0.0\n",
    "#loss_weight[UNK_TOKEN] = 0.0\n",
    "loss_weight[PAD_TOKEN] = 0.0\n",
    "pretrain_loss_fun = nn.NLLLoss(weight=loss_weight)\n",
    "\n",
    "# Create logger instance\n",
    "dis = updatable_display2(['train'],[\"epoch\",\"d_error\",\"g_error\",\"beta\"])\n",
    "pretrain_dis = updatable_display2(['train'],[\"pretrain epoch\",\"pretrain_g_error\"])\n",
    "\n",
    "#print(generator)\n",
    "#print()\n",
    "#print(discriminator)\n",
    "np_g = num_parameters(generator)\n",
    "np_d = num_parameters(discriminator)\n",
    "print(\"Number of parameters for G: {}\\nNumber of parameters for D: {}\\nNumber of parameters in total: {}\"\n",
    "      .format(np_g,np_d,np_g+np_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_generator(real_data,fake_data,optimizer):\n",
    "    '''\n",
    "    Pretrain the generator to generate realistic samples for a good initialization\n",
    "    '''\n",
    "    num_classes = fake_data.size(2)\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "    if fake_data.min() < 0:\n",
    "        print(fake_data.min())\n",
    "        assert False\n",
    "    fake_data = torch.log(fake_data+1e-8)\n",
    "    for i in range(fake_data.size(1)):\n",
    "        loss += pretrain_loss_fun(fake_data[:,i,:],real_data[:,i])\n",
    "    loss /= fake_data.size(1)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(real_data_onehot,fake_data,optimizer):\n",
    "    '''\n",
    "    Train the generator to generate realistic samples and thereby fool the discriminator\n",
    "    '''\n",
    "    N = fake_data.size(0)\n",
    "    \n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1.1 Train on Real Data\n",
    "    c_x_r = discriminator(real_data_onehot)\n",
    "\n",
    "    # 1.2 Train on Fake Data\n",
    "    fake_data_save = fake_data\n",
    "    c_x_f = discriminator(fake_data)\n",
    "        \n",
    "    # compute the average of c_x_*\n",
    "    c_x_r_mean = torch.mean(c_x_r,dim=0)\n",
    "    c_x_f_mean = torch.mean(c_x_f,dim=0)\n",
    "    \n",
    "    loss_real = loss_fun(c_x_r-c_x_f_mean,fake_target(N,device))\n",
    "    loss_fake = loss_fun(c_x_f-c_x_r_mean,true_target(N,device))\n",
    "    \n",
    "    loss = (loss_real + loss_fake)/2.0\n",
    "    if torch.isnan(loss):\n",
    "        print(fake_data_save)\n",
    "        assert False\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(real_data_onehot,fake_data,optimizer):\n",
    "    '''\n",
    "    Train the discriminator to distinguish between real and fake data\n",
    "    '''\n",
    "    N = real_data_onehot.size(0)\n",
    "    \n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 1.1 Train on Real Data\n",
    "    c_x_r = discriminator(real_data_onehot)\n",
    "\n",
    "    # 1.2 Train on Fake Data\n",
    "    fake_data_save = fake_data\n",
    "    c_x_f = discriminator(fake_data)\n",
    "        \n",
    "    # compute the average of c_x_*\n",
    "    c_x_r_mean = torch.mean(c_x_r,dim=0)\n",
    "    c_x_f_mean = torch.mean(c_x_f,dim=0)\n",
    "        \n",
    "    loss_real = loss_fun(c_x_r-c_x_f_mean,true_target(N,device))\n",
    "    loss_fake = loss_fun(c_x_f-c_x_r_mean,fake_target(N,device))\n",
    "    \n",
    "    loss = (loss_real + loss_fake)/2.0\n",
    "    if torch.isnan(loss):\n",
    "        print(fake_data_save)\n",
    "        assert False\n",
    "    loss.backward()\n",
    "    \n",
    "    # 1.3 Update weights with gradients\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e5504de72ce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoise_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         fake_data = generator(z=noise,num_steps=num_steps,temperature=pretrain_temperature,\n\u001b[0;32m---> 30\u001b[0;31m                               x=real_data.long())\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Python/Machine_Learning/GenerativeAdversarialNetworks/Scripts/generators.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, num_steps, temperature, x)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Python/Machine_Learning/GenerativeAdversarialNetworks/Scripts/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, num_steps, temperature, x)\u001b[0m\n\u001b[1;32m    395\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape is [batch,num_steps,output_size,num_heads]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_mem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m                 \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape is [batch,num_steps,output_size,num_heads]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# now output is [batch,num_steps,output_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen_steps = 1\n",
    "gen_train_freq = 1\n",
    "max_temperature = torch.FloatTensor([2]).to(device)\n",
    "pretrain_temperature = torch.FloatTensor([1]).to(device)\n",
    "\n",
    "epochs_pretrain = 100\n",
    "pretrain_step = 0\n",
    "\n",
    "num_epochs = 200\n",
    "global_step = 0\n",
    "epoch = 0\n",
    "d_error = 0\n",
    "g_error = 0\n",
    "\n",
    "# pretrain generator\n",
    "for ep in range(epochs_pretrain):\n",
    "    train_iter = iter(train_data)\n",
    "    batch_error = []\n",
    "    for n_batch,batch in enumerate(train_iter):\n",
    "        real_data = batch.text\n",
    "        N = real_data.size(0)\n",
    "        num_steps = real_data.size(1)\n",
    "        real_data_onehot = onehot(real_data,vocab_size)#.to(device)\n",
    "\n",
    "        # Generate fake data\n",
    "        generator.train()\n",
    "        noise = sample_noise(N,noise_size,device)\n",
    "        fake_data = generator(z=noise,num_steps=num_steps,temperature=pretrain_temperature,\n",
    "                              x=real_data.long())\n",
    "        if torch.isnan(fake_data).any():\n",
    "            print(noise)\n",
    "            assert False\n",
    "        # Train G\n",
    "        pretrain_g_error = pretrain_generator(real_data,fake_data,g_optimizer)\n",
    "        batch_error.append(pretrain_g_error)\n",
    "     \n",
    "        # Log batch error and delete tensors\n",
    "        pretrain_dis.update(pretrain_step,'train',{\"pretrain epoch\":ep,\"pretrain_g_error\":pretrain_g_error.item()})\n",
    "        pretrain_step += 1\n",
    "        if pretrain_step % 200 == 0:\n",
    "            pretrain_dis.display(scale=True)\n",
    "    g_lr_scheduler.step(torch.stack(batch_error).mean())\n",
    "    \"\"\"\n",
    "    if ep % 10 == 0:\n",
    "        generator.eval()\n",
    "        test_samples = generator(z=test_noise,num_steps=num_steps,temperature=pretrain_temperature)\n",
    "        test_samples_vals = torch.argmax(test_samples,dim=2)\n",
    "        plotSamples(test_samples_vals,xu=num_steps,yu=vocab_size,title=\"Fake data\")\n",
    "        plt.show()\n",
    "    \"\"\"\n",
    "        \n",
    "# train adverserially\n",
    "try:\n",
    "    while epoch < num_epochs:\n",
    "        train_iter = iter(train_data)\n",
    "        temperature = max_temperature**((epoch+1)/num_epochs)\n",
    "        for n_batch,batch in enumerate(train_iter):\n",
    "            real_data = batch.text\n",
    "            N = real_data.size(0)\n",
    "            num_steps = real_data.size(1)\n",
    "            # 1. Train Discriminator\n",
    "            real_data_onehot = onehot(real_data,vocab_size)#.to(device)\n",
    "            real_data_onehot[real_data_onehot==1] = 0.9\n",
    "            real_data_onehot[real_data_onehot==0] = (1.0-0.9)/(vocab_size-1.0)\n",
    "\n",
    "            # Generate fake data and detach \n",
    "            # (so gradients are not calculated for generator)\n",
    "            noise_tensor = sample_noise(N,noise_size,device)\n",
    "            with torch.no_grad():\n",
    "                fake_data = generator(z=noise_tensor,num_steps=num_steps,temperature=temperature).detach()\n",
    "            if torch.isnan(fake_data).any():\n",
    "                print(noise_tensor)\n",
    "                assert False\n",
    "            # Train D\n",
    "            d_error = train_discriminator(real_data_onehot,fake_data,d_optimizer)\n",
    "\n",
    "            # 2. Train Generator every 'gen_train_freq' steps\n",
    "            if global_step % gen_train_freq == 0:\n",
    "                for _ in range(gen_steps):\n",
    "                    # Generate fake data\n",
    "                    noise_tensor2 = sample_noise(N,noise_size,device)\n",
    "                    fake_data = generator(z=noise_tensor2,num_steps=num_steps,temperature=temperature)\n",
    "                    if torch.isnan(fake_data).any():\n",
    "                        print(noise_tensor2)\n",
    "                        assert False\n",
    "                    # Train G\n",
    "                    g_error = train_generator(real_data_onehot,fake_data,g_optimizer)\n",
    "                    g_error = g_error.item()\n",
    "\n",
    "            # Log batch error and delete tensors\n",
    "            dis.update(global_step,'train',{\"epoch\":epoch,\"d_error\":d_error.item(),\"g_error\":g_error,\n",
    "                                            \"beta\":temperature.item()})\n",
    "            global_step += 1\n",
    "\n",
    "            # Display Progress every few batches\n",
    "            if global_step % 50 == 0:\n",
    "                #test_samples = generator(z=test_noise,num_steps=num_steps,temperature=temperature)\n",
    "                #test_samples_vals = torch.argmax(test_samples,dim=2)\n",
    "                dis.display(scale=True)\n",
    "                #plotSamples(real_data,xu=num_steps,yu=num_classes,title=\"Real data\")\n",
    "                #plotSamples(test_samples_vals,xu=num_steps,yu=num_classes,title=\"Fake data\")\n",
    "                #if epoch % 50 == 0:\n",
    "                #    plt.savefig(\"Figures/RaSGAN-RNN-ToyData-Epoch=\"+str(epoch)+\".png\")\n",
    "                #plt.show()\n",
    "        epoch += 1\n",
    "except:\n",
    "    #test_samples = generator(z=test_noise,num_steps=num_steps,temperature=temperature)\n",
    "    #test_samples_vals = torch.argmax(test_samples,dim=2)\n",
    "    dis.display(scale=True)\n",
    "    #plotSamples(real_data,xu=num_steps,yu=num_classes,title=\"Real data\")\n",
    "    #plotSamples(test_samples_vals,xu=num_steps,yu=num_classes,title=\"Fake data\")\n",
    "    #plt.savefig(\"Figures/RaSGAN-RNN-ToyData.png\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(generator,model_save_path)\n",
    "#save_model(discriminator,model_save_path)\n",
    "\n",
    "# to load model run\n",
    "#load_model(model,model_load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_gen(real_data,fake_data):\n",
    "    '''\n",
    "    Evaluate the generators ability to generate diverse samples\n",
    "    '''    \n",
    "    loss = 0\n",
    "    fake_data = torch.log(fake_data)\n",
    "    for i in range(fake_data.size(1)):\n",
    "        loss += pretrain_loss_fun(fake_data[:,i,:],real_data[:,i])\n",
    "    loss /= fake_data.size(1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate generator\n",
    "nll_gen_error = []\n",
    "for n_batch,batch in enumerate(val_iter):\n",
    "    real_data = batch.text\n",
    "    N = real_data.size(0)\n",
    "    num_steps = real_data.size(1)\n",
    "\n",
    "    # Generate fake data\n",
    "    generator.eval()\n",
    "    noise = sample_noise(N,noise_size,device)\n",
    "    fake_data = generator(z=noise,num_steps=num_steps,temperature=max_temperature,\n",
    "                          x=real_data.long())\n",
    "    # Train G\n",
    "    nll_g_error = nll_gen(real_data,fake_data)\n",
    "    nll_gen_error.append(nll_g_error)\n",
    "\n",
    "nll_gen_error = torch.stack(nll_gen_error)\n",
    "print(nll_gen_error.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
