{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN for Sequential data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from viz import updatable_display2\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import onehot,num_parameters,sample_noise,true_target,fake_target\n",
    "from layers import GumbelSoftmax\n",
    "from generators import GumbelRelRNNGenerator\n",
    "from discriminators import GumbelSARNNDiscriminator\n",
    "\n",
    "from timeSeries import Sinusoids\n",
    "from visualize import plotSamples\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_steps = 12\n",
    "dataset_size = 10000\n",
    "num_classes = 10\n",
    "\n",
    "data_loader = DataLoader(Sinusoids(num_steps,virtual_size=dataset_size,quantization=num_classes),batch_size=batch_size,shuffle=True)\n",
    "valid_data_loader = DataLoader(Sinusoids(num_steps,virtual_size=dataset_size,quantization=num_classes),batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters for G: 178577\n",
      "Number of parameters for D: 87114\n",
      "Number of parameters in total: 265691\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "\n",
    "vocab_size = num_classes+1\n",
    "pad_id = num_classes\n",
    "noise_size = 128\n",
    "\n",
    "num_test_samples = batch_size\n",
    "test_noise = sample_noise(num_test_samples,noise_size,device)\n",
    "\n",
    "# intialize models\n",
    "generator = GumbelRelRNNGenerator(mem_slots=16,head_size=16,num_heads=8,noise_size=noise_size,output_size=vocab_size,\n",
    "                               device=device).to(device)\n",
    "memory = generator.initMemory(batch_size).to(device)\n",
    "discriminator = GumbelSARNNDiscriminator(input_size=vocab_size,hidden_size=256,output_size=1).to(device)\n",
    "\n",
    "# otpimizers\n",
    "g_optimizer = optim.Adam(generator.parameters(),lr=lr)\n",
    "d_optimizer = optim.Adam(discriminator.parameters(),lr=lr)\n",
    "g_lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(g_optimizer,mode=\"min\",factor=0.5,patience=2,verbose=True)\n",
    "\n",
    "# losses\n",
    "loss_fun = nn.BCEWithLogitsLoss()\n",
    "loss_weight = torch.ones(vocab_size).to(device)\n",
    "loss_weight[pad_id] = 0.0\n",
    "pretrain_loss_fun = nn.NLLLoss(weight=loss_weight)\n",
    "\n",
    "# Create logger instance\n",
    "dis = updatable_display2(['train'],[\"epoch\",\"d_error\",\"g_error\",\"beta\"])\n",
    "pretrain_dis = updatable_display2(['train'],[\"pretrain epoch\",\"pretrain_g_error\"])\n",
    "\n",
    "np_g = num_parameters(generator)\n",
    "np_d = num_parameters(discriminator)\n",
    "print(\"Number of parameters for G: {}\\nNumber of parameters for D: {}\\nNumber of parameters in total: {}\"\n",
    "      .format(np_g,np_d,np_g+np_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_generator(real_data,fake_data,optimizer):\n",
    "    '''\n",
    "    Pretrain the generator to generate realistic samples for a good initialization\n",
    "    '''\n",
    "    num_classes = fake_data.size(2)\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "    fake_data = torch.log(fake_data)\n",
    "    for i in range(fake_data.size(1)):\n",
    "        loss += pretrain_loss_fun(fake_data[:,i,:],real_data[:,i])\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(real_data_onehot,fake_data,optimizer):\n",
    "    '''\n",
    "    Train the generator to generate realistic samples and thereby fool the discriminator\n",
    "    '''\n",
    "    N = fake_data.size(0)\n",
    "    \n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1.1 Train on Real Data\n",
    "    c_x_r = discriminator(real_data_onehot)\n",
    "\n",
    "    # 1.2 Train on Fake Data\n",
    "    c_x_f = discriminator(fake_data)\n",
    "        \n",
    "    # compute the average of c_x_*\n",
    "    c_x_r_mean = torch.mean(c_x_r,dim=0)\n",
    "    c_x_f_mean = torch.mean(c_x_f,dim=0)\n",
    "    \n",
    "    loss_real = loss_fun(c_x_r-c_x_f_mean,fake_target(N,device))\n",
    "    loss_fake = loss_fun(c_x_f-c_x_r_mean,true_target(N,device))\n",
    "    \n",
    "    loss = (loss_real + loss_fake)/2.0\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(real_data_onehot,fake_data,optimizer):\n",
    "    '''\n",
    "    Train the discriminator to distinguish between real and fake data\n",
    "    '''\n",
    "    N = real_data_onehot.size(0)\n",
    "    \n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 1.1 Train on Real Data\n",
    "    c_x_r = discriminator(real_data_onehot)\n",
    "\n",
    "    # 1.2 Train on Fake Data\n",
    "    c_x_f = discriminator(fake_data)\n",
    "        \n",
    "    # compute the average of c_x_*\n",
    "    c_x_r_mean = torch.mean(c_x_r,dim=0)\n",
    "    c_x_f_mean = torch.mean(c_x_f,dim=0)\n",
    "        \n",
    "    loss_real = loss_fun(c_x_r-c_x_f_mean,true_target(N,device))\n",
    "    loss_fake = loss_fun(c_x_f-c_x_r_mean,fake_target(N,device))\n",
    "    \n",
    "    loss = (loss_real + loss_fake)/2.0\n",
    "    loss.backward()\n",
    "    \n",
    "    # 1.3 Update weights with gradients\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f108d3aaf909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                               x=real_data.long(),memory=memory)\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Train G\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mpretrain_g_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mbatch_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrain_g_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7570f825d943>\u001b[0m in \u001b[0;36mpretrain_generator\u001b[0;34m(real_data, fake_data, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpretrain_loss_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen_steps = 1\n",
    "gen_train_freq = 1\n",
    "max_temperature = torch.FloatTensor([2]).to(device)\n",
    "pretrain_temperature = torch.FloatTensor([1]).to(device)\n",
    "\n",
    "epochs_pretrain = 100\n",
    "pretrain_step = 0\n",
    "\n",
    "num_epochs = 200\n",
    "global_step = 0\n",
    "epoch = 0\n",
    "d_error = 0\n",
    "g_error = 0\n",
    "\n",
    "# pretrain generator\n",
    "for ep in range(epochs_pretrain):\n",
    "    batch_error = []\n",
    "    for n_batch,real_batch in enumerate(data_loader):\n",
    "        N = real_batch.size(0)\n",
    "        real_batch = real_batch.squeeze(2)\n",
    "        real_data = real_batch.to(device)\n",
    "        real_data_onehot = onehot(real_batch,vocab_size).to(device)\n",
    "\n",
    "        # Generate fake data\n",
    "        generator.train()\n",
    "        noise = sample_noise(N,noise_size,device)\n",
    "        fake_data,memory = generator(z=noise,num_steps=num_steps,temperature=pretrain_temperature,\n",
    "                              x=real_data.long(),memory=memory)\n",
    "        # Train G\n",
    "        pretrain_g_error = pretrain_generator(real_data,fake_data,g_optimizer)\n",
    "        batch_error.append(pretrain_g_error)\n",
    "        \n",
    "        del fake_data\n",
    "        del real_data\n",
    "        \n",
    "        # Log batch error and delete tensors\n",
    "        pretrain_dis.update(pretrain_step,'train',{\"pretrain epoch\":ep,\"pretrain_g_error\":pretrain_g_error.item()})\n",
    "        pretrain_step += 1\n",
    "        if pretrain_step % 200 == 0:\n",
    "            pretrain_dis.display(scale=True)\n",
    "    g_lr_scheduler.step(torch.stack(batch_error).mean())\n",
    "    if ep % 10 == 0:\n",
    "        generator.eval()\n",
    "        test_samples,memory = generator(z=test_noise,num_steps=num_steps,temperature=pretrain_temperature,memory=memory)\n",
    "        test_samples_vals = torch.argmax(test_samples,dim=2)\n",
    "        plotSamples(test_samples_vals,xu=num_steps,yu=vocab_size,title=\"Fake data\")\n",
    "        plt.show()\n",
    "        \n",
    "# train adverserially\n",
    "try:\n",
    "    while epoch < num_epochs:\n",
    "        temperature = max_temperature**((epoch+1)/num_epochs)\n",
    "        for n_batch,real_batch in enumerate(data_loader):\n",
    "            N = real_batch.size(0)\n",
    "            # 1. Train Discriminator\n",
    "            real_batch = real_batch.squeeze(2)\n",
    "            real_data = real_batch.to(device)\n",
    "            real_data_onehot = onehot(real_batch,vocab_size).to(device)\n",
    "            real_data_onehot[real_data_onehot==1] = 0.9\n",
    "            real_data_onehot[real_data_onehot==0] = (1.0-0.9)/(vocab_size-1.0)\n",
    "\n",
    "            # Generate fake data and detach \n",
    "            # (so gradients are not calculated for generator)\n",
    "            noise_tensor = sample_noise(N,noise_size,device)\n",
    "            with torch.no_grad():\n",
    "                fake_data,memory = generator(z=noise_tensor,num_steps=num_steps,\n",
    "                                             temperature=temperature,memory=memory).detach()\n",
    "            # Train D\n",
    "            d_error = train_discriminator(real_data_onehot,fake_data,d_optimizer)\n",
    "\n",
    "            # 2. Train Generator every 'gen_train_freq' steps\n",
    "            if global_step % gen_train_freq == 0:\n",
    "                for _ in range(gen_steps):\n",
    "                    # Generate fake data\n",
    "                    fake_data,memory = generator(z=sample_noise(N,noise_size,device),num_steps=num_steps,\n",
    "                                                 temperature=temperature,memory=memory)\n",
    "                    # Train G\n",
    "                    g_error = train_generator(real_data_onehot,fake_data,g_optimizer)\n",
    "                    g_error = g_error.item()\n",
    "\n",
    "            # Log batch error and delete tensors\n",
    "            dis.update(global_step,'train',{\"epoch\":epoch,\"d_error\":d_error.item(),\"g_error\":g_error,\n",
    "                                            \"beta\":temperature.item()})\n",
    "            global_step += 1\n",
    "            del fake_data\n",
    "            del noise_tensor\n",
    "\n",
    "            # Display Progress every few batches\n",
    "            if global_step % 50 == 0:\n",
    "                test_samples,memory = generator(z=test_noise,num_steps=num_steps,temperature=temperature,memory=memory)\n",
    "                test_samples_vals = torch.argmax(test_samples,dim=2)\n",
    "                dis.display(scale=True)\n",
    "                plotSamples(real_data,xu=num_steps,yu=num_classes,title=\"Real data\")\n",
    "                plotSamples(test_samples_vals,xu=num_steps,yu=num_classes,title=\"Fake data\")\n",
    "                if epoch % 50 == 0:\n",
    "                    plt.savefig(\"Figures/RaSGAN-Rel-RNN-ToyData-Epoch=\"+str(epoch)+\".png\")\n",
    "                plt.show()\n",
    "            del real_data\n",
    "        epoch += 1\n",
    "except:\n",
    "    test_samples,memory = generator(z=test_noise,num_steps=num_steps,temperature=temperature,memory=memory)\n",
    "    test_samples_vals = torch.argmax(test_samples,dim=2)\n",
    "    dis.display(scale=True)\n",
    "    plotSamples(real_data,xu=num_steps,yu=num_classes,title=\"Real data\")\n",
    "    plotSamples(test_samples_vals,xu=num_steps,yu=num_classes,title=\"Fake data\")\n",
    "    plt.savefig(\"Figures/RaSGAN-Rel-RNN-ToyData.png\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
